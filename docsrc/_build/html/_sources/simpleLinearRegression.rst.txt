==========================
Simple Linear Regression
==========================

Many applications of statistics are much more complex than:
- Estimating a mean or variance
- Testing if the population mean or variance is equal to some value
- Testing if two means are equal

Often we will use sample data to investigate the relationship between two or more variables. The ultimate goal is to create a model that can be used to predict the value of a single variable.

Regression Analysis
====================

The process of finding a mathematical model (or equation) that best fits the data is part of a statistical technique known as Regression Analysis.
In regression analysis, the variable to be modeled or predicted is called the dependent variable, or response variable.

Y = the dependent variable

The variables that are used to model Y are called independent variables, or regressors.

:math:`X_i:` = independent variable

If we have multiple independent variables (:math:`X_1, X_2, ... , X_k:`) the analysis of finding the best model is called multiple regression.
We will start with a model where we have only one independent variable, X. With only one X, we have simple linear regression because the model will form a simple straight line equation.

A reasonable form of a relationship between Y and X is the linear relationship:

:math:`Y = Œ≤_0+Œ≤_1 ùëã`

:math:`Œ≤_0` is the intercept
:math:`Œ≤_1` is the slope
This is called a deterministic model because if :math:`Œ≤_0` and :math:`Œ≤_1` are known for the population, then the value of X determines exactly the value of Y, there is no random or probabilistic component. 

In the real world, we will not be able to determine :math:`Œ≤_0` and :math:`Œ≤_1` exactly and there will be some components that cannot be measured or explained. Thus, there will be some random component in the equation.

:math:`Y = Œ≤_0+Œ≤_1 ùëã + ùúÄ` which is called a probabilistic model.

ùúÄ is called random error.

Error
======

Random Error is assumed to have the following properties:

- E(ùúÄ) = 0
- Var(ùúÄ) = œÉ2 (homogenous variance assumption)

.. image:: _static/images/errorReg.png
   :width: 400
   :height: 300

Fitted Regression Line
=======================

We try to estimate the regression equation by obtaining sample values of X and Y. We use this sample information to estimate the values of  :math:`Œ≤_0` and :math:`Œ≤_1` with :math:`b_0` and :math:`b_1`. Since E(ùúÄ) = 0, we do not estimate error because it is expected to 
be zero over time. Our estimated or fitted regression line is given by:

.. math::

   \hat{y} = b_0 + b_1 * X

This is an estimate of the ‚Äútrue‚Äù regression line. When a large amount of data is available, we expect the fitted line to be close to the true regression line. We can never draw the true regression line because this would require 
that we sample the entire population.

Estimating Parameters
----------------------

How do we come up with values for  :math:`b_0` and :math:`b_1` ?

**Example**

Suppose an appliance store conducts a 5-month experiment to determine the effect of advertising on sales revenue.

:math:`Y = Œ≤_0+Œ≤_1 ùëã + ùúÄ`

Y = sales revenue in thousands of dollars

X = advertising expenditures in hundreds of dollars

Œµ = random error

Sample data is collected for different values of advertising expenditure. Sample data is of the form: {(Xi, Yi); i = 1, ‚Ä¶ n} We will use the sample data to estimate the true regression line with the fitted line :math:`\hat{y} = b_0 + b_1 * X`
This equation will allow us to compute predicted values of Y for the observed values of X.

Residual
---------

Now, for each value of :math:`X_i`, we have our sample value :math:`y_i` and our predicted value, :math:`\hat{y}_i`, obtained from the fitted line. The ith residual is defined to be the difference between the observed and predicted values.

Residual: :math:`e_i = y_i - \hat{y}_i`    for i = 1, 2, ‚Ä¶, n

If our fitted model closely matches the sample data, the residuals will be small and we will have a good fit.

Thus, when finding our estimates, :math:`b_0` and :math:`b_1` , we wish to minimize the residuals. Since residuals can be positive or negative a common measure for looking at total residual value is called ‚ÄúSum of Squares of the Errors‚Äù and is denoted SSE.

SSE is found by summing the squared residuals. Instead of minimizing residuals directly, we will attempt to minimize SSE. This minimization procedure for estimating the parameters is called the Method of Least Squares.

Method Of Least Squares
========================

.. math::

   \sum{e_i^2} = \sum{(y_i - b_0 - b_1x_i)^2}

In order to minimize SSE we differentiate SSE with respect to :math:`b_0` and :math:`b_1` . The result is 

.. math::

   b_1 = \frac{\sum{(x_i-\bar{x})(y_i-\bar{y})}}{\sum{(x_i-\bar{x})^2}}

and 

.. math::

   b_0 = \bar{y} - b_1\bar{x}


**Example**

Suppose an appliance store conducts a 5-month experiment to determine the effect of advertising (X) on sales revenue (Y).
Sample data is:

============ ============
    X             Y
============ ============
   1               1
   2               1
   3               2
   4               2
   5               4
============ ============

Then 

============ ============ ============ ============
    X             Y           X^2         XY
============ ============ ============ ============
   1               1          1             1
   2               1          4             2
   3               2          9             6
   4               2          16            8
   5               4          25            20
   **15**        **10**       **55**      **37**
============ ============ ============ ============

:math:`\bar{x} = 3`

:math:`\bar{y} = 2`

:math:`n = 5`

Then

.. math::
   
   b_1 = \frac{\sum{(x_i-\bar{x})(y_i-\bar{y})}}{\sum{(x_i-\bar{x})^2}} = \frac{5(37)-(15)(10)}{5(55)-(15)^2} = 0.7

.. math::
   
   b_0 = \bar{y} - b_1\bar{x} = 2-(0.7)3=-0.1

Then the least Squares fitted line is :math:`\hat{y} = -0.1 + 0.7x` . We can now calculate the Sum of Squares of the Errors (SSE)


============ ============ ============ ============
    X             Y            yHat     (y-yhat)^2
============ ============ ============ ============
   1               1          0.6           0.16
   2               1          1.3           0.09
   3               2          2.0           0
   4               2          2.7           0.49
   5               4          3.4           0.36
============ ============ ============ ============

Then SSE = :math:`\sum{(y_i - \hat{y_i})^2} = 1.1`

Interpretations
================

:math:`Œ≤_0` = y-intercept of the line or the point at which the line intercepts the y-axis

:math:`Œ≤_1` = slope of the line or the amount of increase (or decrease) in the mean of y for every 1-unit increase in x.

In our example

:math:`b_0` = -0.1 which is the point where the fitted line crosses the y-axis

:math:`b_1` = 0.7 = the mean monthly sales revenue increases $700 for every $100 increase in monthly advertising expenditure. (since y is measured in units of $1,000 and x in units of $100)

What is good about least squares?
----------------------------------

It Minimizes the sum of squares of vertical deviations from the sample data points to the fitted line. There are other ways to measure closeness, like the sum of the absolute value of the residuals.

.. math::

   \sum{|y_i - \hat{y_i}|}

but all methods force the residuals to be small.

Properties of least squares
----------------------------

- Estimates  b0 and b1 are based on given sample
- If the experiment is repeated over and over, the estimates b0 and b1 will differ from experiment to experiment 
- The least square estimators are both UNBIASED estimators
- :math:`E(b_0) = Œ≤_0`
- :math:`E(b_1) = Œ≤_1`

More sum of squares equations
==============================

We have already defined SSE, sum of squares of error. Further sum of squares formulas will become necessary in future calculations:

:math:`S_{xx}` = sum of squares of x = :math:`\sum{x^2_i - n(\bar{x})^2}`

:math:`S_{yy}` = sum of squares of y = :math:`\sum{y^2_i - n(\bar{y})^2}`

:math:`S_{xy}` = sum of squares of x and y = :math:`\sum{x_iy_i - n\bar{y}\bar{x}}`

The following equalitites are also true

:math:`b_1 = \frac{S_{xy}}{S_{xx}}`

:math:`SEE = S_{syy} - b_1S_{xy}`


Model Assumptions
==================

When we perform least squares regression we make the following assumptions about random error, Œµ

- The mean of Œµ is 0
   - i.e. E(Œµ) = 0 
- The variance of Œµ is constant for all x
   - Var(Œµ) = œÉ2
- The probability distribution of Œµ is normal
   - Œµ ~ Normal
- The error terms are independent of one another

Checking model assumptions
----------------------------

Graphical methods and statistical tests can be used to check the validity of our assumptions on error. We will use two graphs to check the first three assumptions. (Assumption of independence can also be verified with another graph. We will not be testing this in our class.)

Normal Probability plot 
------------------------

In a normal probability plot the residuals are graphed against the expected values of the residuals under the assumption of normality. If the normality assumption is valid, the plot should resemble a straight line, sloping upward to the right. If the assumption is not
valid, you will often see the pattern fail in the tails of the graph, or create an obvious curve away from the straight line.

.. image:: _static/images/nnp.png
   :width: 400
   :height: 300

Assumption of normality is valid

.. image:: _static/images/nnpBad.png
   :width: 400
   :height: 300

Assumption of normality is invalid

Residuals vs fits
------------------

A graph that plots the residual values ( :math:`e_i` ) versus the fitted values of the regression ( :math:`\hat{y}` ) can be used to check if the expected value of error is zero and if the assumption of homogenous variance is valid. If the expected value of error is zero, the plot should have most of its values 
around zero. If the assumption of equal variances is valid there should be no distinct pattern in the plot; we should see ‚Äúrandom scatter‚Äù.

.. image:: _static/images/resFits.png
   :width: 400
   :height: 300

The figure above displays random scatter which has most of the values relatively around zero. We would conclude that the equal variance assumption and the assumption that the expected value of error is zero, are both valid.
   
.. image:: _static/images/resFitsBad.png
   :width: 400
   :height: 300

Figure 2 appears to have most of the values around zero, so our first assumption is valid. However, the plot has a distinct triangular pattern. Any pattern suggests that the equal variance assumption is not valid.

Estimation of Model Error variance
===================================

An unbiased estimator of :math:`\sigma^2` is:

.. math::

   S^2 = \frac{SSE}{n-2} = \frac{S_{yy} - b_1S_{xy}}{n-2}

This is also called the mean square error (MSE)

Estimated Standard deviation
-----------------------------

The standard deviation, S, measures the spread of the distribution of Y about the fitted least squares line. We can expect 95% of the observed Y values to lie within 2S of their respective least square predicted values 


Inferences Concerning Regression Coefficients
===============================================

Inferences on  :math:`ùõΩ_1`
----------------------------

We can estimate the linear relationship between X and Y by computing values :math:`b_0` and :math:`b_1` as our estimates for intercept and slope. We could also be interested in drawing inferences about the slope and intercept themselves.
What are possible values that the slope could take on? How useful is X in predicting Y?

If our assumption of normality of error is valid, it follows that the Y values are also normally distributed, and thus so is ùõΩ_1.
From these assumptions we can create the statistic:

.. math::

   T = \frac{Œ≤_1}{s/\sqrt{S_{XX}}}

This statistic will follow a t-distribution with  v = n ‚Äì 2 degrees of freedom.

A 100(1 - ùõº)% confidence interval for the parameter :math:`ùõΩ_1` in the regression line is:

.. math::

   b_1 \pm t_{\alpha/2}\frac{S}{\sqrt{S_{xx}}}

Where :math:`ùë°_{ùõº/2}` is a value from the t-distribution with v = n ‚Äì 2 degrees of freedom.
From this interval we can be 100(1 - ùõº)% confident that the slope of the true population regression line lies in this interval.

The Quantity:

.. math::

   \frac{s}{\sqrt{S_{xx}}}

Is called the standard error of :math:`ùõΩ_1`

**Example**

Recall the appliance store example where we wish to determine the effect of advertising (X) on sales revenue (Y) using a 5 month experiment.
Previously we came up the a fitted regression line of :math:`\hat{y}= -0.1 + 0.7x`

Our estimated value of slope was 0.7. Find a 95% confidence interval for slope.

Using the data from the appliance store example

:math:`S_{xx} = 10` 

:math:`SEE = 1.1`

:math:`S^2 = \frac{SEE}{n-2} = \frac{1.1}{5-2} = 0.366667`

S = :math:`\sqrt{S^2} = 0.6055`           

:math:`b_1 = 0.7`       

:math:`t_{\alpha/2} = t_{0.025} = 3.182 (V = 5-2 = 3)`

.. math::

   b_1 \pm t_{\alpha/2}\frac{S}{\sqrt{S_{xx}}}

.. math::

   0.7 \pm 3.182(\frac{0.6055}{\sqrt{10}})

.. math::

   0.7 \pm 0.609

(0.091, 1.309)

0.091 < :math:`ùõΩ_1` < 1.309

IE We are 95% sure that B1 is positive

Hypothesis Test on :math:`ùõΩ_1`
--------------------------------

A common question once the regression analysis has been complete is ‚ÄúDoes X truly influence Y?‚Äù If X does not influence Y (X is not useful in the prediction of Y), then the value of Y does not change regardless of the value of X. 
This implies that the slope of the line, ùõΩ_1, is zero. So to test if X is influencing Y we use the hypotheses:

:math:`ùõΩ_1 = 0`  vs :math:`ùõΩ_1 = 0`

To test whether the slope equals zero we use the test statistic:

.. math::

   T = \frac{b_1}{s/\sqrt{S_{XX}}}

Which follows a t-distribution with v = n ‚Äì 2. Since we are testing against a two-sided alternative (Ha: ùõΩ_1 ‚â† 0), we will reject the null hypothesis if:
   
.. math::

   |T| > t_{\alpha/2}

Interpreting the results of this hypothesis test. If we reject H0: ùõΩ_1 = 0, then we conclude that the relationship between X an Y is linear and that X does contribute information to the prediction of Y. (The linear model is useful).
If we do not reject H0: ùõΩ_1 = 0, there is no linear relationship between X and Y. Changing X has little impact on changes in Y. (X does not contribute information to the prediction of Y)

**Example**

Use the appliance store example and test if X contributes significant information to the prediction of Y at a 0.05 level of significance.
From the previous example:

n = 5     

:math:`ùõΩ_{xx} = 10`       

:math:`S = 0.6055`      

:math:`b_1 = 0.7`     

:math:`H_0: ùõΩ_1 = 0`             

:math:`H_0: ùõΩ_1 \ne 0`

.. math::

   T = \frac{b_1}{s/\sqrt{S_{xx}}} = \frac{0.7}{0.6055/\sqrt{10}} = 3.66

Critical value: :math:`t_{\alpha/2} = t_{0.025} = 3.182` (v=3)

Reject :math:`H_0` if |T| > 3.182

3.66 > 3.182

Reject H0. There is sufficient evidence, at the 0.05 level, that advertising expenditure contributes information to predicting sales revenue.

Note:
If the confidence interval for ùõΩ_1 does not contain zero we can conclude that the true value of ùõΩ_1 is not zero and we can reject the null hypothesis H0: ùõΩ_1 = 0.

Inferences on :math:`ùõΩ_0`
---------------------------

We are often more interested in inferences about slope than those on the y-intercept, but it is possible to create confidence intervals and hypothesis tests for ùõΩ_0. These inferences are based on the statistic:

.. math:: 

   T = \frac{ùõΩ_{0}}{s\sqrt{\sum{x^2_i/(nS_{xx})}}}

Which follows a t-distribution with v = n ‚Äì 2.

Confidence Interval for :math:`ùõΩ_0`
------------------------------------

A 100(1 - ùõº)% confidence interval for the parameter ùõΩ_0 in the regression line is:

.. math::

   b_0 \pm t_{a/2} \frac{S\sqrt{\sum{x^2_i}}}{\sqrt{nS_{xx}}}

Where ùë°_(ùõº/2) is a value from the t-distribution with v = n ‚Äì 2 degrees of freedom.
From this interval we can be 100(1 - ùõº)% confident that the intercept of the true population regression line lies in this interval.

Hypothesis Test on :math:`ùõΩ_0`
-------------------------------

When testing the hypotheses :math:`ùõΩ_1 = 0`  vs :math:`ùõΩ_1 = 0` at the :math:`` level of significance, the test statistic is: 

.. math::

   T = \frac{b_0}{S\sqrt{\sum{x^2_i/(nS_{xx})}}}

We will reject :math:`H_0` if |T| > :math:`t_\alpha/2` with :math:`t_\alpha/2` from a t-distribution with v = n - 2.

**Example**

Create a 95% confidence interval for ùõΩ_0 from the appliance store example.

n = 5

:math:`b_0` = -0.1

S_{xx} = 10

:math:`\sum{x^2_i}` = 55

S = 0.6055

:math:`t_{\alpha/2} = 3.182` (v=3)

.. math::

   b_0 \pm t_{a/2} \frac{S\sqrt{\sum{x^2_i}}}{\sqrt{nS_{xx}}}

.. math::

   -0.1 \pm 3.182 \frac{0.6055\sqrt{55}}{\sqrt{5(10)}}

.. math::

   -0.1 \pm 2.02

-2.12 < :math:`ùõΩ_0` < 1.92

Coefficient of Correlation
===========================

Previously we have discussed the coefficient of determination (R2) which measures the amount of variation in Y that is explained by the model. Correlation implies a relationship or association between two variables.
The coefficient of correlation is a measure of strength of the linear relationship between X and Y. 

The population correlation coefficient is denoted as ùúå   (Greek letter rho) We estimate this value with the sample correlation coefficient

.. math::

   r = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}} = b_1\sqrt{\frac{S_{xx}}{S_{yy}}}

Properties
1. r will have the same sign as :math:`b_1`
2. :math:`-1 \ge r \ge 1`

Interpretations
----------------

1. If r = 0 there is no linear relationship between X and Y.
2. If r = 1 or -1 then all of the points lie exactly on the least squares line. (Prefect linear fit)
3. If r is close to 1 or -1 there is a strong linear relationship.
4. If r is positive then Y increases as X increases.
5. If r is negative then Y decreases as X increases.


.. image:: _static/images/noCorrelationR.png
   :width: 400
   :height: 300

No Correlation. r = 0

.. image:: _static/images/perfectLinearR.png
   :width: 400
   :height: 300

Perfect linear relationship r = 1

.. image:: _static/images/strongPositiveR.png
   :width: 400
   :height: 300

Strong Positive correlation, r = 0.931

.. image:: _static/images/weakNegativeR.png
   :width: 400
   :height: 300

Weaker negative correlation, r = -0.67

**Example**

An appliance store conducts a 5-month experiment to determine the effect of advertising (X) on sales revenue (Y). From previous work:

:math:`S_{xy} = 7`

:math:`S_{xx} = 10`

:math:`S_{yy} = 6`

Find an estimate for the coefficient of correlation.

.. math::

   r = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}} = \frac{7}{\sqrt{(10)(6)}} = 0.904

There is a strong positive linear correlation between X and Y.

Also note that :math:`R^2 = (r)^2 = 0.904^2 = 0.817`

Estimation and Prediction
==========================

The fitted line can be used for two purposes:
- Estimating the average value of Y for a given value of X. (estimating E(Y))
- Predicting a particular value of Y for a given value of X. (predicting y0)

Note: when estimating or predicting we are doing so for a certain value of X. This value of X does not have to be in the sample data.

Estimation
-----------

Suppose we wish to construct a confidence interval for the mean response of Y at a specific value of X. (confidence interval for E(Y) at X = x0) The best estimate we have for E(Y) is by plugging in x0 into our fitted line.

.. math::

   \hat{y}_0 = b_0 + b_1x_0

A 100(1 ‚Äì Œ±)% confidence interval for the mean response E(Y) at a given X = x0 is:

.. math::

   \hat{y}_0 \pm t_{\alpha/2}S\sqrt{\frac{1}{n}+\frac{(x_0 - \bar{x})^2}{S_{xx}}}

Where :math:`t_{\alpha/2}` is a value from the t-distribution with v = n ‚Äì 2.

**Example**

Use the data from the appliance store example to create a 95% confidence interval for the mean response of Y when X = 5.

From Previous work we know

:math:`\hat{y} = -0.1 + 0.7x`

S = 0.6055

:math:`S_{xx}` = 10

:math:`\bar{x}` = 3

n = 5

:math:`t_{\alpha/2}` = :math:`t_{0.025}` = 3.182 (v = 5-2 = 3)

So 

.. math::

   \hat{y}_0 = b_0 + b_1x_0 = -0.1 + 0.7(5) = 3.4

.. math::

   3.4 \pm 3.182(0.6055)\sqrt{\frac{1}{5}+\frac{(5-3)^2}{10}}

.. math::

   3.4 \pm 1.492 = (1.908, 4.892)

Prediction
------------

We are interested in predicting a specific value of Y instead of just obtaining an estimate for the mean value. Particular values of Y are more difficult to predict and thus require a 
wider range of values in the interval. There is some additional error in the prediction because of the deviation of Y from the line of means. The error in using the fitted line to
estimate the line of means The error caused by deviation of y from the line of means, measure by :math:`œÉ^2` 

Prediction Interval for Y
--------------------------

A 100(1 ‚Äì Œ±)% prediction interval for a single response y0 at a given X = x0 is:

.. math::

   \hat{y}_0 \pm t_{\alpha/2}S\sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}}

**Example**

Using the same information from example 1, we can create a prediction interval for y0 when X = 5, in the appliance store problem.

.. math::

   3.4 \pm 3.182(0.6055)\sqrt{1 + \frac{1}{5}+\frac{(5-3)^2}{10}}

.. math::

   3.4 \pm 2.437 = (0.963, 5.837)

Interpretations of confidence and prediction intervals:

- The confidence interval interpretation is the same as the one we studied earlier in ch.9
   - We are 100(1 ‚Äì Œ±)% confidence that the true mean response is within the interval
- The prediction interval represents an interval that has a probability of (1 ‚Äì Œ±) of containing a future value of y0 when X = x0

Confidence vs. Prediction Intervals

- Confidence bands are narrower than the prediction bands for every value of X.
- Predictions are always more variable than the estimates of the mean value.
- The bands will be wider as the value of x0 gets farther from ùë•¬†ÃÖ.
- In practice, estimation and prediction are more accurate when x0 is near the center of the range of the x-values.

.. image:: _static/images/confidenceBands.png
   :width: 400
   :height: 300

Confidence Bands

















   