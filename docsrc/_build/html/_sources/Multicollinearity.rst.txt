====================
Multicollinearity
====================

Multicollinearity exists when two or more of the independent variables used in the multiple regression model are highly correlated.
This will cause serious problems in analyzing the model.

Problems with Multicollinearity
--------------------------------

If multicollinearity exists in the model we will see these effects on the analysis:

- The estimated regression coefficients will have large standard errors, causing imprecision in confidence and prediction intervals.
- Adding or deleting a predictor variable may cause significant changes in the values of the other regression coefficients.

Detecting Multicollinearity
-----------------------------

How can you tell whether a regression analysis exhibits multicollinearity?
We will have confusing regression results.

1. The individual T-tests are insignificant but the overall F-test is significant.
    - Meaning the individual variables are all insignificant but our model is useful.
2. The signs of the regression coefficients are contrary to what you would intuitively expect the contributions of those variables to be.
    - Ex. We are analyzing weight (Y) and find that height has a negative coefficient value.
3. A matrix of correlations, generated by the computer shows you which predictor variables are highly correlated with each other.
    - Correlations close to 1 suggest multicollinearity is present.
4. The variance inflation factor (VIF) for a particular independent variable is greater than 10.
    - VIF values are given when regression analysis is done using computer software

Dealing with multicollinearity
-------------------------------

What do we do if we detect multicollinearity in our model?

1. Drop one or more of the correlated independent variables from the model
2. Use the model for prediction only, not for inferences of individual variables. (only if overall F is significant)
3. Add interaction terms between the correlated variables.


**Example**

The table below is a portion of output for a multiple regression analysis.

Overall F value has p-value = 0.001

=================== =================== =================== =================== ===================
       Term           Estimated value           SE                 P Value                 VIF 
=================== =================== =================== =================== ===================
        ùõΩ_0                0.58                 0.607               0.056               
        ùõΩ_1                2.712                0.202               0.237               13  
        ùõΩ_2                2.05                 0.048               0.522               50
=================== =================== =================== =================== ===================



Test for linearity
====================

Repeated observations
------------------------

In certain kinds of experiments the researcher has the capability of obtaining repeated observations on the response for each value of X.
These repetitions are not necessary to estimate the beta parameters, but they do provide more information that will allow us to determine the appropriateness of the model.

Consider a random sample of n observations with k distinct values of X.

X1, X2, ‚Ä¶., Xk

The sample contains n1 observed values of Y1 corresponding to X1, n2 observed values of Y2 corresponding to X2, and so on.

Thus, n = n1 + n2 + ‚Ä¶ nk 

We have the following notation:

:math:`Y_{ij}` = the the jth value of :math:`Y_{i}` = the jth observation of Y at :math:`X_{i}` 

.. math::
    Y_i = T_i = \sum_{j=i}^{n_i} y_{ij}

.. math::
    \bar{Y_i} = \frac{T_i}{n_i}

**Example Data:**

============= =============
     X              Y
============= =============
    X_1            Y_11
    X_1            Y_12
    X_1            Y_13
    X_2            Y_21
    X_2            Y_22
    X_3            Y_31
    X_3            Y_32
============= =============

Then :math:`Y_1 = T_1 = Y_{11} + Y_{12} + Y_{13} `

Measures of Error
------------------

Error Sum of Squares consists of two parts:

Lack-of-fit: a measure of the systematic variation brought about by higher order terms.

Pure experimental error: variation between the values of Y within given values of X. (random variation)

If we are fitting a simple linear model, we do not have higher order terms and are essentially assuming that the lack-of-fit contribution does not exist. In this case SSE is completely due to random error.
If this is the case, then s2 = SSE/(n-2) is an unbiased estimate of œÉ2.

However, if the model does not adequately fit the data, then SSE is inflated and produces a biased estimate of œÉ2.

Unbiased Estimate of œÉ2 with repeated observations
---------------------------------------------------

If our data has repeated observations an unbiased estimate of œÉ2 can be calculated as:

.. math::
    S^2 = \frac{\sum_{i=1}^{k}\sum_{j=1}^{n_1}(y_{ij}-\bar{y_i})^2}{n-k}

The numerator represents a measure of the pure experimental error.

If our data has repeated observations we can represent lack-of-fit in the ANOVA table to test the linear model‚Äôs adequacy.

1. Compute SSE, SST, and SSR as you normally would for a simple linear model.
2. Compute the pure error sum of squares
    - :math:`\sum_{i=1}^{k}\sum_{j=1}^{n_i}(y_{ij}-\bar{y_i})^2`
    - This will have (n ‚Äì k) degrees of freedom.
3. Subtract pure error SS from SSE to obtain the SS due to lack of fit. This will have (n ‚Äì 2) - (n ‚Äì k) = k ‚Äì 2 degrees of freedom.


==================== ==================== ==================== ==================== ====================
       Source               DF                    SS                    MS                  F
==================== ==================== ==================== ==================== ====================
    Regression              1                     SSR                   SSR
    Error                   n-2                   SSE   
    Lack of fit             k-2               SSE - SSE(pure)        (see note 1)       (see note 2)
    Pure error              n-k               SSE(pure)                 S^2
    Total                   n-1               SST
==================== ==================== ==================== ==================== ====================

Note 1: :math:`\frac{SSE-SEE(Pure)}{k-2}`

:math:`S^2 = \frac{SSE(pure)}{n-k}`

Note 2: :math:`\frac{SSE-SEE(Pure)}{(k-2)s^2}`

Testing for Model Adequacy
---------------------------

Once we have fit a simple linear model and created an ANOVA table which shows the breakdown of error into lack-of-fit, we can use the F-value to test if the linear model is adequate.

:math:`H_0` : linear model is adequate

:math:`H_a` : higher order terms are needed

:math:`F = \frac{SSE-SEE(pure)}{(k-2)s^2}`

Critical value: ùëì_ùõº with v1 = (k-2), v2 = (n-k)

Reject H0 if F > ùëì_ùõº

If we do not reject H0, then we conclude that the first order model is adequate.

**Example**

Observations of the yield of a chemical reaction taken at various temperatures are recorded in the table on the next slide. Estimate the linear model Y = Œ≤0 + Œ≤1X + Œµ and test for lack of fit.

============= =============
     Y              X
============= =============
    77.4            150
    76.7            150
    78.2            150
    84.1            200
    84.5            200
    83.7            200
    88.9            250
    89.2            250
    89.7            250
    94.8            300
    94.7            300
    95.9            300
============= =============

:math:`\bar{ùë•}` = 225 

:math:`\bar{y}` = 86.483 

:math:`S_{xx}` = 645000 ‚Äì :math:`12(225)^2` = 37500

:math:`S_{xy}` = 237875 ‚Äì 12(225)(86.483) = 4370

:math:`b_1` = 4370/37500=0.1165 

:math:`b_0` = 86.483 ‚Äì (0.1165)225 = 60.263

:math:`\hat{y}` = 60.263+0.1165ùëã

============= ============= ============= =============
     Y              X             xy            X^2
============= ============= ============= =============
    77.4            150          11610        22500
    76.7            150          11505        22500
    78.2            150          11730        22500
    84.1            200          16820        40000
    84.5            200          16900        40000
    83.7            200          16740        40000
    88.9            250          22225        62500
    89.2            250          22300        62500
    89.7            250          22425        62500
    94.8            300          28440        90000
    94.7            300          28410        90000
    95.9            300          28770        90000
============= ============= ============= =============

The totals for each collumns are 1037.8, 2700, 237875, 645000

:math:`S_{xy}` = 90265.52 ‚Äì :math:`12(86.483)^2` = 513.1167

:math:`SSE` = 513.1167 ‚Äì 0.1165(4370) = 3.866

============= ============= =============
     Y              X             y^2    
============= ============= =============
    77.4            150        5990.76    
    76.7            150        5882.89    
    78.2            150        6115.24 
    84.1            200        7072.81
    84.5            200        7140.25
    83.7            200        7005.69
    88.9            250        7903.21
    89.2            250        7956.64
    89.7            250        8046.09
    94.8            300        8987.04
    94.7            300        8968.09
    95.9            300        9196.81
============= ============= =============

The totals for each collumns are 1037.8, 2700, 90265.52

Sum of Squares:

SST = 513.1167

SSE = 3.866

SSR = SST ‚Äì SSE = 509.251

Finding lack-of-fit:
---------------------

X1 = 150     X2 = 200      X3 = 250     X4 = 300

.. math::
    \bar{y_1} = \frac{y_{11}+y_{12}+y_{13}}{3} = \frac{77.4+ 76.7 + 78.2}}{3} = 77.43

.. math::
    \bar{y_2} =  \frac{84.1 + 84.5 + 83.7}{3} = 84.1

.. math::
    \bar{y_3} =  \frac{88.9+89.2+89.7}{3} = 89.27

.. math::
    \bar{y_4} =  \frac{94.8+94.7+95.9}{3} = 95.13


SSE(pure) = :math:`(77.4 ‚Äì 77.43)^2 + (76.7 ‚Äì 77.43)^2  + ‚Ä¶ + (94.7 ‚Äì 95.13)^2 + (95.9 ‚Äì 95.13)^2` = 2.66

Lack-of-fit = SSE  - SSE(pure) = 3.866 ‚Äì 2.66 = 1.206

==================== ==================== ==================== ==================== ====================
       Source               DF                    SS                    MS                  F
==================== ==================== ==================== ==================== ====================
    Regression              1                 509.251                   509.251             1531.58
    Error                   10                3.866                                         
    Lack of fit             2                 1.206                     0.603               1.81
    Pure error              8                 2.66                      0.3325
    Total                   11                513.1167
==================== ==================== ==================== ==================== ====================


Testing Lack-of-Fit:

H0: the linear model is adequate

Ha: higher order terms are needed

F = 1.81

Critical value ùëì_ùõº = 4.46     with v1 = 2,  v2 = 8

1.18 < 4.46

Do not reject H0. The linear model is adequate.

** Not Complete**
