====================
Multicollinearity
====================

Multicollinearity exists when two or more of the independent variables used in the multiple regression model are highly correlated.
This will cause serious problems in analyzing the model.

Problems with Multicollinearity
--------------------------------

If multicollinearity exists in the model we will see these effects on the analysis:

- The estimated regression coefficients will have large standard errors, causing imprecision in confidence and prediction intervals.
- Adding or deleting a predictor variable may cause significant changes in the values of the other regression coefficients.

Detecting Multicollinearity
-----------------------------

How can you tell whether a regression analysis exhibits multicollinearity?
We will have confusing regression results.

1. The individual T-tests are insignificant but the overall F-test is significant.
    - Meaning the individual variables are all insignificant but our model is useful.
2. The signs of the regression coefficients are contrary to what you would intuitively expect the contributions of those variables to be.
    - Ex. We are analyzing weight (Y) and find that height has a negative coefficient value.
3. A matrix of correlations, generated by the computer shows you which predictor variables are highly correlated with each other.
    - Correlations close to 1 suggest multicollinearity is present.
4. The variance inflation factor (VIF) for a particular independent variable is greater than 10.
    - VIF values are given when regression analysis is done using computer software

Dealing with multicollinearity
-------------------------------

What do we do if we detect multicollinearity in our model?

1. Drop one or more of the correlated independent variables from the model
2. Use the model for prediction only, not for inferences of individual variables. (only if overall F is significant)
3. Add interaction terms between the correlated variables.


**Example**

The table below is a portion of output for a multiple regression analysis.

Overall F value has p-value = 0.001

=================== =================== =================== =================== ===================
       Term           Estimated value           SE                 P Value                 VIF 
=================== =================== =================== =================== ===================
        ùõΩ_0                0.58                 0.607               0.056               
        ùõΩ_1                2.712                0.202               0.237               13  
        ùõΩ_2                2.05                 0.048               0.522               50
=================== =================== =================== =================== ===================



Test for linearity
====================

Repeated observations
------------------------

In certain kinds of experiments the researcher has the capability of obtaining repeated observations on the response for each value of X.
These repetitions are not necessary to estimate the beta parameters, but they do provide more information that will allow us to determine the appropriateness of the model.

Consider a random sample of n observations with k distinct values of X.

X1, X2, ‚Ä¶., Xk

The sample contains n1 observed values of Y1 corresponding to X1, n2 observed values of Y2 corresponding to X2, and so on.

Thus, n = n1 + n2 + ‚Ä¶ nk 

We have the following notation:

:math:`Y_{ij}` = the the jth value of :math:`Y_{i}` = the jth observation of Y at :math:`X_{i}` 

.. math::
    Y_i = T_i = \sum_{j=i}^{n_i} y_{ij}

.. math::
    \bar{Y_i} = \frac{T_i}{n_i}

**Example Data:**

============= =============
     X              Y
============= =============
    X_1            Y_11
    X_1            Y_12
    X_1            Y_13
    X_2            Y_21
    X_2            Y_22
    X_3            Y_31
    X_3            Y_32
============= =============

Then :math:`Y_1 = T_1 = Y_{11} + Y_{12} + Y_{13} `

Measures of Error
------------------

Error Sum of Squares consists of two parts:

Lack-of-fit: a measure of the systematic variation brought about by higher order terms.

Pure experimental error: variation between the values of Y within given values of X. (random variation)

If we are fitting a simple linear model, we do not have higher order terms and are essentially assuming that the lack-of-fit contribution does not exist. In this case SSE is completely due to random error.
If this is the case, then s2 = SSE/(n-2) is an unbiased estimate of œÉ2.

However, if the model does not adequately fit the data, then SSE is inflated and produces a biased estimate of œÉ2.

Unbiased Estimate of œÉ2 with repeated observations
---------------------------------------------------

If our data has repeated observations an unbiased estimate of œÉ2 can be calculated as:

.. math::
    S^2 = \frac{\sum_{i=1}^{k}\sum_{j=1}^{n_1}(y_{ij}-\bar{y_i})^2}{n-k}

The numerator represents a measure of the pure experimental error.

If our data has repeated observations we can represent lack-of-fit in the ANOVA table to test the linear model‚Äôs adequacy.

1. Compute SSE, SST, and SSR as you normally would for a simple linear model.
2. Compute the pure error sum of squares
    - :math:`\sum_{i=1}^{k}\sum_{j=1}^{n_i}(y_{ij}-\bar{y_i})^2`
    - This will have (n ‚Äì k) degrees of freedom.
3. Subtract pure error SS from SSE to obtain the SS due to lack of fit. This will have (n ‚Äì 2) - (n ‚Äì k) = k ‚Äì 2 degrees of freedom.


==================== ==================== ==================== ==================== ====================
       Source               DF                    SS                    MS                  F
==================== ==================== ==================== ==================== ====================
    Regression              1                     SSR                   SSR
    Error                   n-2                   SSE   
    Lack of fit             k-2               SSE - SSE(pure)        (see note 1)       (see note 2)
    Pure error              n-k               SSE(pure)                 S^2
    Total                   n-1               SST
==================== ==================== ==================== ==================== ====================

Note 1: :math:`\frac{SSE-SEE(Pure)}{k-2}`

:math:`S^2 = \frac{SSE(pure)}{n-k}`

Note 2: :math:`\frac{SSE-SEE(Pure)}{(k-2)s^2}`

Testing for Model Adequacy
---------------------------

Once we have fit a simple linear model and created an ANOVA table which shows the breakdown of error into lack-of-fit, we can use the F-value to test if the linear model is adequate.

:math:`H_0` : linear model is adequate

:math:`H_a` : higher order terms are needed

:math:`F = \frac{SSE-SEE(pure)}{(k-2)s^2}`

Critical value: ùëì_ùõº with v1 = (k-2), v2 = (n-k)

Reject H0 if F > ùëì_ùõº

If we do not reject H0, then we conclude that the first order model is adequate.

**Example**

Observations of the yield of a chemical reaction taken at various temperatures are recorded in the table on the next slide. Estimate the linear model Y = Œ≤0 + Œ≤1X + Œµ and test for lack of fit.

============= =============
     Y              X
============= =============
    77.4            150
    76.7            150
    78.2            150
    84.1            200
    84.5            200
    83.7            200
    88.9            250
    89.2            250
    89.7            250
    94.8            300
    94.7            300
    95.9            300
============= =============

:math:`\bar{ùë•}` = 225 

:math:`\bar{y}` = 86.483 

:math:`S_{xx}` = 645000 ‚Äì :math:`12(225)^2` = 37500

:math:`S_{xy}` = 237875 ‚Äì 12(225)(86.483) = 4370

:math:`b_1` = 4370/37500=0.1165 

:math:`b_0` = 86.483 ‚Äì (0.1165)225 = 60.263

:math:`\hat{y}` = 60.263+0.1165ùëã

============= ============= ============= =============
     Y              X             xy            X^2
============= ============= ============= =============
    77.4            150          11610        22500
    76.7            150          11505        22500
    78.2            150          11730        22500
    84.1            200          16820        40000
    84.5            200          16900        40000
    83.7            200          16740        40000
    88.9            250          22225        62500
    89.2            250          22300        62500
    89.7            250          22425        62500
    94.8            300          28440        90000
    94.7            300          28410        90000
    95.9            300          28770        90000
============= ============= ============= =============

The totals for each collumns are 1037.8, 2700, 237875, 645000

:math:`S_{xy}` = 90265.52 ‚Äì :math:`12(86.483)^2` = 513.1167

:math:`SSE` = 513.1167 ‚Äì 0.1165(4370) = 3.866

============= ============= =============
     Y              X             y^2    
============= ============= =============
    77.4            150        5990.76    
    76.7            150        5882.89    
    78.2            150        6115.24 
    84.1            200        7072.81
    84.5            200        7140.25
    83.7            200        7005.69
    88.9            250        7903.21
    89.2            250        7956.64
    89.7            250        8046.09
    94.8            300        8987.04
    94.7            300        8968.09
    95.9            300        9196.81
============= ============= =============

The totals for each collumns are 1037.8, 2700, 90265.52

Sum of Squares:

SST = 513.1167

SSE = 3.866

SSR = SST ‚Äì SSE = 509.251

Finding lack-of-fit:
---------------------

X1 = 150     X2 = 200      X3 = 250     X4 = 300

.. math::
    \bar{y_1} = \frac{y_{11}+y_{12}+y_{13}}{3} = \frac{77.4+ 76.7 + 78.2}{3} = 77.43

.. math::
    \bar{y_2} =  \frac{84.1 + 84.5 + 83.7}{3} = 84.1

.. math::
    \bar{y_3} =  \frac{88.9+89.2+89.7}{3} = 89.27

.. math::
    \bar{y_4} =  \frac{94.8+94.7+95.9}{3} = 95.13


SSE(pure) = :math:`(77.4 ‚Äì 77.43)^2 + (76.7 ‚Äì 77.43)^2  + ‚Ä¶ + (94.7 ‚Äì 95.13)^2 + (95.9 ‚Äì 95.13)^2` = 2.66

Lack-of-fit = SSE  - SSE(pure) = 3.866 ‚Äì 2.66 = 1.206

==================== ==================== ==================== ==================== ====================
       Source               DF                    SS                    MS                  F
==================== ==================== ==================== ==================== ====================
    Regression              1                 509.251                   509.251             1531.58
    Error                   10                3.866                                         
    Lack of fit             2                 1.206                     0.603               1.81
    Pure error              8                 2.66                      0.3325
    Total                   11                513.1167
==================== ==================== ==================== ==================== ====================


Testing Lack-of-Fit:

H0: the linear model is adequate

Ha: higher order terms are needed

F = 1.81

Critical value ùëì_ùõº = 4.46     with v1 = 2,  v2 = 8

1.18 < 4.46

Do not reject H0. The linear model is adequate.

One Factor experiments
-----------------------

In our confidence intervals and hypothesis tests we were restricted to considering no more than 2 populations at once.

- CI for ¬µ1 - ¬µ2 

- H0: ¬µ1 = ¬µ2

This is called the one-factor problem.

Ex. Running times of films produced by 2 motion picture companies are compared.

One Factor = company (2 levels -> company 1 and company 2)

If we wanted to compare more companies we would have 1 factor but more levels and more samples.

If the number of levels, k, is greater than 2 we will assume there are k samples from k populations.

A common procedure to deal with testing population means is called Analysis of Variance, or ANOVA.

**Example**

Suppose we wish to compare average running time of films from 4 different production companies. It is decided that 5 films from each company will be sampled. So we have a total of 20 sample observations.
There are 4 populations. We may wish to test:
- H0: ¬µ1 = ¬µ2 = ¬µ3 = ¬µ4
- Ha: at least 2 means are not equal

Or we could be interested in making individual comparisons among the 4 populations.

=============== =============== =============== =============== ===============
 Company                1              2              3                 4
=============== =============== =============== =============== ===============
                      102              81              87              98
                      86               165             114             117
                      98               97              94              109
                      109              134             100             91
                      92               92              107             93
                      -                -                -               -
    Total:            487             569             502             508
    Mean:             97.4            113.8           100.4           101.6
=============== =============== =============== =============== ===============

Sources of Variability
------------------------

In the analysis of variance procedure, it is assumed that whatever variation exists between the averages is attributed to:
- Variation in film length among observations within the companies
- Variation among companies (due to differences between the different companies)
- The within-sample variation is considered to be chance or random variation.

We wish to determine if the difference among the 4 sample means are what we would expect due to random variation alone.

Random samples of size n are selected from each of k populations. The k different populations are classified on the basis of a single criterion (i.e. different treatments or groups)
We use the term treatment to generally refer to various classifications.
It is assumed that the k populations are independent and normally distributed with means ¬µ1, ¬µ2, ‚Ä¶, ¬µk and common variance œÉ2

Let yij denote the jth observation from the ith treatment.

:math:`Y_i` = total of all observations from ith treatment = :math:`\sum^{n}_{j=1}{y_{ij}}`

:math:`\bar{Y_i}` = sample mean of observations from ith treatment = :math:`\frac{y_i}{n}`                                                               
	
:math:`Y_{..}` = total of all nk observations = :math:`\sum^{k}_{i=i}\sum^{n}_{j=1}{y_{ij}}`

:math:`\bar{Y_{..}}` = mean of all nk observations = :math:`\frac{y_{..}}{nk}`

=============== =============== =============== =============== ==========================
 Treatment             1              i                k
=============== =============== =============== =============== ==========================
                      Y_11          y_i1             y_k1
                      Y_12          y_i2             y_k2
                        .             .               .
                        .             .               .
                        .             .               .
                      Y_1n            y_in           y_kn
                      -                -                -             -
    Total:              Y_1             Y_i           Y_K               :math:`Y_..`
    Mean:             Ybar1          Ybari           Ybark          :math:`\bar{Y_{..}}`
=============== =============== =============== =============== ==========================

Each observation can be written as: :math:`Y_{ij} = ¬µ_i + Œµ_{ij}`

Where :math:`Œµ_{ij}` measures the deviation of the jth observation from its treatment mean = random error

We can also write: 	:math:`¬µ_i = ¬µ + Œ±_i`

Where

- :math:`Œ±_i` = effect of ith treatment    ( :math:`\sum^{k}_{i=1}{a_i=0}` )
- ¬µ = grand mean= (:math:`\sum^{k}_{i=1}{u_i}`)/k

Thus :math:`Y_{ij} = ¬µ + Œ±_i + Œµ_{ij}` 

Now we can write

H0: ¬µ1 = ¬µ2 = ‚Ä¶ = ¬µk 

Ha: at least 2 means are not equal

Or

H0: Œ±1 = Œ±2 = ‚Ä¶ = Œ±k = 0

Ha: at least 1 Œ±i is not zero

Partitioning Variability
-------------------------

Total Sum of Squares = SST = :math:`\sum^{k}_{i=i}\sum^{n}_{j=1}{(y_{ij}-\bar{y_{..}})^2}`

Treatment Sum of Squares = SSA = :math:`n\sum^{k}_{i=i}(\bar{y_{i.}}-\bar{y_{..}})^2`

Error Sum of Squares = SSE = :math:`\sum^{k}_{i=i}\sum^{n}_{j=1}{(y_{ij}-\bar{y_{i.}})^2}`

Sum of Square Identity: SST = SSA + SSE

=================== =================== =================== ================================= =================================
Source                     SS                   DF                    MS                                F 
=================== =================== =================== ================================= =================================
Regression                  SSA                  k-1         :math:`S^2_1=\frac{SSA}{k-1}`     :math:`\frac{s^2_1}{s^2}`
ERROR                       SSE                  k(n-1)      :math:`S^2=\frac{SSE}{k(n-1)}`
TOTAL                       SST                  nk-1
=================== =================== =================== ================================= =================================

s2 is an unbiased estimate of œÉ2 (common population variance). It is unbiased regardless of the truth or falsity of H0: ¬µ1= ‚Ä¶ = ¬µk 

ùë†_1^2 provides an unbiased estimate of œÉ2 when H0 is true. If Ha is true, ùë†_1^2 will estimate œÉ2 with an extra term to measure variations due to the treatment effects (Œ±i‚Äôs).

So if Ha is true ùë†_1^2 > s2 and F will be large.

Testing Multiple populations
------------------------------

H0: ¬µ1 = ¬µ2 = ‚Ä¶ = ¬µk 

Ha: at least 2 means are not equal

Test statistic: F = (ùë†_1^2)/ùë†^2 

Reject H0 if F > fŒ±[(k ‚Äì 1), k(n ‚Äì 1)]

If you reject H0, there is evidence that a difference exists in the population means for the k treatments.

We wish to compare average running time of films from 4 different production companies.
It is decided that 5 films from each company will be sampled.

=============== =============== =============== =============== =============== ===============
 Company                1              2              3                 4
=============== =============== =============== =============== =============== ===============
                      102              81              87              98
                      86               165             114             117
                      98               97              94              109
                      109              134             100             91
                      92               92              107             93
                      -                -                -               -
    Total:            487             569             502             508           2066
    Mean:             97.4            113.8           100.4           101.6         103.3
=============== =============== =============== =============== =============== ===============

Test if there is a difference in the average running times for the different companies at 0.05 level of significance.

k = 4       n = 5

.. math::
    SST = \sum^{k}_{i=i}\sum^{n}_{j=1}{(y_{ij}-\bar{y_{..}})^2}
        = (102 ‚Äì 103.3)^2 + (86 ‚Äì 103.3)^2 + ‚Ä¶ +  (93 ‚Äì 103.3)^2   
        = 6900.2

(nk = 20 terms in the summation)

.. math::   
    SSA = n\sum^{k}_{i=i}(\bar{y_{i.}}-\bar{y_{..}})^2
    = 5[ (97.4 ‚Äì 103.3)^2 + (113.8 ‚Äì 103.3)^2 + (100.4 ‚Äì 103.3)^2 + (101.6 ‚Äì 103.3)^2 ]
    = 781.8
    
(k = 4 terms in the summation)

.. math::
    SSE = \sum^{k}_{i=i}\sum^{n}_{j=1}{(y_{ij}-\bar{y_{i.}})^2}
    = (102 ‚Äì 97.4)^2 + (86 ‚Äì 97.4)^2 + ‚Ä¶ + (91 ‚Äì 101.6)^2 + (93 ‚Äì 101.6)^2 
    = 6118.4

(nk = 20 terms in summation)

=================== =================== =================== ================================= =================================
Source                     SS                   DF                    MS                                F 
=================== =================== =================== ================================= =================================
Regression                  781.8               3            :math:`S^2_1=260.6`                  0.681
ERROR                       SSE                  k(n-1)      :math:`S^2=382.4`
TOTAL                       SST                  nk-1
=================== =================== =================== ================================= =================================

H0: ¬µ1 = ¬µ2 = ¬µ3 = ¬µ4

Ha: at least 2 means are not equal

Test statistic: F = 0.681

Critical value: f0.05(3, 16) = 3.24

0.681 < 3.24   ÔÉ† Do not reject H0

There is not evidence that the average running times for these 4 companies are significantly different at Œ± = 0.05.

Unequal Sample Sizes
---------------------

We are not always able to obtain equal sample sizes from all populations. (experimental data is lost/ damaged, humans drop out of the study, etc.)

The process of testing for equivalent means will be the same, but we need to adjust the SS formulas to take into account the different sample sizes.

k random samples

Sizes n1, n2, ‚Ä¶, nk

N = n1 + n2 + ‚Ä¶ + nk 

ùëÜùëÜùëá= :math:`\sum^{k}_{i=i}\sum^{n_i}_{j=1}{(y_{ij}-\bar{y_{..}})^2}`                   ùëëùëì=ùëÅ‚àí1

ùëÜùëÜùê¥= :math:`\sum^{k}_{i=i}n_i(\bar{y_{i.}}-\bar{y_{..}})^2`                       ùëëùëì=ùëò‚àí1

ùëÜùëÜùê∏=ùëÜùëÜùëá ‚àíùëÜùëÜùê¥                                 ùëëùëì=ùëÅ‚àíùëò

